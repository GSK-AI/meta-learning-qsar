#!/usr/bin/env python3
"""Train MAML from featurized data

For description of available argument:
    python src/train_maml.py --help

Usage:
    python src/train_maml.py \\
        --save_path <directory to store checkpoint> \\
        --source <directory where training and validation data is stored> \\
        ...

Note:
    - The directory provided to --source must contain 3 files:
        - featurized_data.pkl: Pickled data for graphs. Generated by src/featurize.py
        - meta_split_idx.pkl: Pickled task splits dictionary with schema:
            {
                "meta_train": {
                    "train": [train_task_idx1, train_task_idx2,...],
                    "val": [val_task_idx1, val_task_idx2,...],
                    "test": [test_task_idx1, test_task_idx2,...]
                },
                "meta_val": {...},
                "meta_test": {...}
            }
        - dataset_split_idx.pkl: Pickled splits dictionary with schema similar to meta_split_idx.pkl.
          In the example below, [task1_train_cmpd_idx1, task1_train_cmpd_idx2 ...] are compound indices 
          for train_task_idx1 above.
            {
                "meta_train": {
                    "train": [
                        [task1_train_cmpd_idx1, task1_train_cmpd_idx2 ...],
                        [task2_train_cmpd_idx1, task2_train_cmpd_idx2,...],
                        ...
                    ],
                    "val": [
                        [task1_val_idx1, task1_val_idx2 ...],
                        [task2_val_idx1, task2_val_idx1,...],
                        ...
                    ],
                    "test": [...]
                },
                "meta_val": {...},
                "meta_test": {...}
            }

"""
import copy
import gc
import json
import os
import pickle
import random
import sys
from functools import partial

import learn2learn as l2l
import numpy as np
import torch
from absl import app, flags, logging
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split
from torch import nn, optim
from torchvision import transforms
from torchvision.datasets import ImageFolder

from src.models.ggnn import GatedGraphNeuralNetwork
from src.models.l2l_maml import MAML
from src.training.meta import meta_training
from src.utils import dataloaders, torch_utils

FLAGS = flags.FLAGS
# Save path
flags.DEFINE_string("save_path", None, "Misc: Folder directory for saving MAML models")
# GGNN architecture hyperparameters
flags.DEFINE_integer("n_conv", 7, "Architecture: Number of gated graph convolution layers")
flags.DEFINE_integer("fc_dims", 1024, "Architecture: Number of fully connected layers")
# Data source for training
flags.DEFINE_string("source", None, "Training: Data folder (see docstrings for requirements)")
# Training hyperparameters
flags.DEFINE_integer("seed", 0, "Training: Random seed")
flags.DEFINE_string("mode", "binary_classification", "Training: [regression, binary_classification]")
flags.DEFINE_float("meta_lr", 0.005, "Training: Meta learning rate")
flags.DEFINE_integer("meta_batch_size", 32, "Training: Meta batch size")
flags.DEFINE_float("inner_lr", 0.1, "Training: Inner loop learning rate")
flags.DEFINE_integer("inner_batch_size", 64, "Training: Inner loop batch size")
flags.DEFINE_integer("inner_steps", 2, "Training: Number of gradient steps to take in inner loop")
flags.DEFINE_bool("first_order", False, "Training: Use first order approximation in MAML")
flags.DEFINE_bool("weight_norm", False, "Training: Apply weight normalization to model weights")
flags.DEFINE_bool("anil", False, "Training: Use the ANIL algorithm from DeepMind")
flags.DEFINE_integer("meta_steps", 60000, "Training: Number of meta gradient steps to take")
flags.DEFINE_string("init_path", None, "Training: Path for model initialization")
flags.DEFINE_integer("ckpt_steps", 100, "Training: Number of iterations between checkpoints")
flags.DEFINE_string("metrics", "accuracy_score", "Training: Metrics to capture during training")
# Additional notes for training run
flags.DEFINE_string("notes", None, "Misc: Notes for training run")

flags.mark_flag_as_required("save_path")
flags.mark_flag_as_required("source")


def main(argv):
    logging.info(f"Starting MAML training with {FLAGS.source} dataset.")
    ckpt_save_path = os.path.join(FLAGS.save_path, "ckpts")
    os.makedirs(ckpt_save_path, exist_ok=True)

    logging.info(f"Setting seed...")
    torch_utils.set_seed(FLAGS.seed)

    metadata = [f.serialize() for f in FLAGS.get_key_flags_for_module(sys.argv[0])]
    metadata = [m for m in metadata if m]  # remove empty flags
    metadata = "\n\t" + "\n\t".join(metadata)
    logging.info(f"Current parameters: {metadata}")

    flag_file = os.path.join(ckpt_save_path, "flagfile.txt")
    FLAGS.flags_into_string()
    FLAGS.append_flags_into_file(flag_file)
    logging.info(f"Flags are stored to {flag_file}")

    logging.info("Loading data...")
    loaders = dataloaders.get_loaders(
        source_path=FLAGS.source, inner_batch_size=FLAGS.inner_batch_size
    )

    logging.info("Instantiating model and optimizers...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = GatedGraphNeuralNetwork(
        n_edge=1, in_dim=75, n_conv=FLAGS.n_conv, fc_dims=[FLAGS.fc_dims, 1], p_dropout=0.0
    )
    if FLAGS.init_path is not None:
        logging.info(f"Loading initializations from {FLAGS.init_path}")
        model = torch.load(FLAGS.init_path)
    model = model.to(device)
    meta_learner = MAML(model, lr=FLAGS.inner_lr, first_order=FLAGS.first_order, anil=FLAGS.anil)

    optimizer = optim.Adam(meta_learner.parameters(), FLAGS.meta_lr)
    if FLAGS.mode == "binary_classification":
        pos_weight = torch.tensor(
            [l.dataset.y.sum() / len(l.dataset.y) for l in loaders["meta_train"]["train"]]
        ).mean()
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    elif FLAGS.mode == "regression":
        criterion = nn.MSELoss()
    else:
        logging.error(f"--mode {FLAGS.mode} is not supported. Choose from ['binary_classification', 'regression'].")
        sys.exit(1)

    metrics = FLAGS.metrics.split(",") if FLAGS.metrics else []

    logging.info(f"Begin training!")
    meta_training(
        meta_learner=meta_learner,
        meta_steps=FLAGS.meta_steps,
        meta_batch_size=FLAGS.meta_batch_size,
        loaders=loaders,
        optimizer=optimizer,
        criterion=criterion,
        inner_steps=FLAGS.inner_steps,
        device=device,
        save_path=ckpt_save_path,
        ckpt_steps=FLAGS.ckpt_steps,
        metrics=metrics,
    )


if __name__ == "__main__":
    app.run(main)
